{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_config = {\n",
    "    'data_directory': 'data',\n",
    "    'config_directory': 'configs',\n",
    "    'model_directory': 'models',\n",
    "    'log__directory': 'logs',\n",
    "    \"compression\": \"gzip\",\n",
    "    \"data_format\": \"csv\",\n",
    "    \"select_columns\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "    \"data_type_dict\": {\"number\": [\"a\", \"b\", \"c\", \"d\"], \"string\": [\"e\", \"f\", \"g\"], \"date\": [\"h\", \"i\"]}\n",
    "}\n",
    "\n",
    "parsing_config = {\n",
    "    \"date_format_configs\":  [{\"date_column\": \"h\", \"format\": \"%Y-%m-%d\"},{\"date_column\": \"i\", \"format\": \"%Y-%m-%d\"}],\n",
    "    \"input_data_precision\": 2,\n",
    "    \"output_data_precision\": 2,\n",
    "    \"optimized_data_schema\": \"optimized_data_schema.json\",\n",
    "}\n",
    "\n",
    "validator_config = {\n",
    "    \"column_check\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "    \"range_check\": {\"a\": [0, 100], \"b\": [0, 100], \"c\": [0, 100], \"d\": [0, 100]},\n",
    "    \"unique_check\": {\"a\":[], \"b\":[], \"c\":[], \"d\":[]},\n",
    "    \"null_check\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "    \"cardinality_check\": [\"a\", \"b\"],\n",
    "}\n",
    "\n",
    "\n",
    "pre_processor_config = {\n",
    "    'imputation': {'a': 'mean', 'b': 'mean', 'c': 'mean', 'd': 'mean'},\n",
    "    'model_granularity': ['a','b'],\n",
    "    'date_granularity': [{'date_col':'MS','date_col2':'D'}],\n",
    "    'aggregate': {'a': ['sum', 'mean'], 'b': ['sum', 'mean'],'target': ['max']},\n",
    "    'req_col_mapper': {'poc_id': 'a', 'order_id': 'b','quantity': 'c', 'date': 'd', 'target': 'e'},\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1. read sample data: use `select_cols` for reading data. example: sample_df = pd.read_csv('data.csv', usecols=select_cols, nrows=1000)\n",
    "# 2. parse date columns according to `date_configs`\n",
    "# 3. then optimized the data set so that we can get a `full_data_types_dict`.\n",
    "# 4. read full data using `full_data_types_dict` and `select_cols`: optimized_data = pd.read_csv(path, data_types_dict=full_data_types_dict, usecols=select_cols).rename(columns=rename_cols)\n",
    "# 5. rename_cols: should be for mandatory columns. We need to have assert statement to check if all mandatory columns are present in the data set. We should not allow other columns to be renamed.\n",
    "# 6. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_processor_config = {\n",
    "    'req_col_mapper': [{'poc_id': 'a', 'order_id': 'b','quantity': 'c', 'date': 'd', 'target': 'e'}],\n",
    "    'model_granularity': ['a','b'],\n",
    "    'date_granularity': [{'date_col':'MS','date_col2':'D'}],\n",
    "    'aggregate': [{'a': ['sum', 'mean'], 'b': ['sum', 'mean'],'target': ['max']}],\n",
    "    }\n",
    "\n",
    "\n",
    "# write a function to preprocss the data based on the config\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function which can check a list of folders present in local or not. If not present then create them.\n",
    "import os\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "def create_folders(folder_list):\n",
    "    for folder in folder_list:\n",
    "        create_folder(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(df,date_col_day_mapping):\n",
    "    for date_col,date_mapping in date_col_day_mapping.items():\n",
    "        if date_mapping =='M':\n",
    "            df[date_col].apply(lambda x: x.replace(day=1))\n",
    "        elif date_mapping =='W':\n",
    "            df[date_col] = df[date_col] - timedelta(days = df[date_col].weekday())\n",
    "        elif date_mapping =='D':\n",
    "            continue\n",
    "        elif date_mapping =='Y':\n",
    "            df[date_col]  = df[date_col].replace(day=1,month=1)\n",
    "        else:\n",
    "            raise ValueError('date mapping not supported')\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_model_granularity(df,model_granularity):\n",
    "    if len(model_granularity)>1:\n",
    "        colname = '_'.join(model_granularity)\n",
    "        df[colname] = df[model_granularity[0]].astype(str)\n",
    "        for col in model_granularity[1:]:\n",
    "            df[colname] = df[colname].astype(str) + \"||\" + df[col].astype(str)\n",
    "    else:\n",
    "        colname = model_granularity[0]\n",
    "    return df,colname\n",
    "\n",
    "\n",
    "def aggregate_data(df,model_granularity_col_name,aggregate_config,date_col):\n",
    "    df = df.copy()\n",
    "    if date_col:\n",
    "        df = df.groupby([model_granularity_col_name,date_col]).agg(aggregate_config)\n",
    "    else:\n",
    "        df = df.groupby([model_granularity_col_name]).agg(aggregate_config)\n",
    "    df.columns = [\"_\".join(x) for x in df.columns.tolist()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor_config = {\n",
    "    'req_col_mapper': {'poc_id': 'a', 'order_id': 'b','quantity': 'c', 'date': 'date1', 'target': 'e'},\n",
    "    'imputation': {'a': 'mean', 'b': 'mean', 'c': 'mean', 'd': 'mean'},\n",
    "    'model_granularity': ['a','b'],\n",
    "    'date_granularity': [{'date1':'M'}],\n",
    "    'aggregate': {'a': ['sum', 'mean'], 'b': ['sum', 'mean'],'target': ['max']},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'req_col_mapper': {'poc_id': 'a',\n",
       "  'order_id': 'b',\n",
       "  'quantity': 'c',\n",
       "  'date': 'date1',\n",
       "  'target': 'e'},\n",
       " 'imputation': {'a': 'mean', 'b': 'mean', 'c': 'mean', 'd': 'mean'},\n",
       " 'model_granularity': ['a', 'b'],\n",
       " 'date_granularity': [{'date1': 'M'}],\n",
       " 'aggregate': {'a': ['sum', 'mean'], 'b': ['sum', 'mean'], 'target': ['max']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processor_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processor_pipe(df,pre_processor_config):\n",
    "    df = convert_date(df,pre_processor_config['date_granularity'][0])\n",
    "    df,model_granularity_col_name = create_model_granularity(df,pre_processor_config['model_granularity'])\n",
    "    df = aggregate_data(df,model_granularity_col_name,pre_processor_config['aggregate'],pre_processor_config['req_col_mapper']['date'])\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/09 00:42:47 WARN Utils: Your hostname, ab-inbev resolves to a loopback address: 127.0.1.1; using 192.168.0.101 instead (on interface wlp0s20f3)\n",
      "23/03/09 00:42:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/09 00:42:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'assdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/frame.py:11886\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m  11885\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m> 11886\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[:, key]\n\u001b[1;32m  11887\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/indexing.py:480\u001b[0m, in \u001b[0;36mLocIndexerLike.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    473\u001b[0m cond, limit, remaining_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_rows(rows_sel)\n\u001b[1;32m    474\u001b[0m (\n\u001b[1;32m    475\u001b[0m     column_labels,\n\u001b[1;32m    476\u001b[0m     data_spark_columns,\n\u001b[1;32m    477\u001b[0m     data_fields,\n\u001b[1;32m    478\u001b[0m     returns_series,\n\u001b[1;32m    479\u001b[0m     series_name,\n\u001b[0;32m--> 480\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols(cols_sel)\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m cond \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m returns_series:\n",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/indexing.py:327\u001b[0m, in \u001b[0;36mLocIndexerLike._select_cols\u001b[0;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_select_cols_else(cols_sel, missing_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/indexing.py:1373\u001b[0m, in \u001b[0;36mLocIndexer._select_cols_else\u001b[0;34m(self, cols_sel, missing_keys)\u001b[0m\n\u001b[1;32m   1372\u001b[0m     cols_sel \u001b[39m=\u001b[39m (cols_sel,)\n\u001b[0;32m-> 1373\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_from_multiindex_column(cols_sel, missing_keys)\n",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/indexing.py:1191\u001b[0m, in \u001b[0;36mLocIndexer._get_from_multiindex_column\u001b[0;34m(self, key, missing_keys, labels, recursed)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m missing_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1191\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(k)\n\u001b[1;32m   1192\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'assdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pre_processor_pipe(data,pre_processor_config)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mpre_processor_pipe\u001b[0;34m(df, pre_processor_config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_processor_pipe\u001b[39m(df,pre_processor_config):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[39m=\u001b[39m convert_date(df,pre_processor_config[\u001b[39m'\u001b[39;49m\u001b[39mdate_granularity\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m     df,model_granularity_col_name \u001b[39m=\u001b[39m create_model_granularity(df,pre_processor_config[\u001b[39m'\u001b[39m\u001b[39mmodel_granularity\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m     df \u001b[39m=\u001b[39m aggregate_data(df,model_granularity_col_name,pre_processor_config[\u001b[39m'\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m'\u001b[39m],pre_processor_config[\u001b[39m'\u001b[39m\u001b[39mreq_col_mapper\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m, in \u001b[0;36mconvert_date\u001b[0;34m(df, date_col_day_mapping)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m date_col,date_mapping \u001b[39min\u001b[39;00m date_col_day_mapping\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      3\u001b[0m     \u001b[39mif\u001b[39;00m date_mapping \u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mM\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m         df\u001b[39m.\u001b[39;49massdf[date_col]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(day\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m      5\u001b[0m     \u001b[39melif\u001b[39;00m date_mapping \u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mW\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m         df[date_col] \u001b[39m=\u001b[39m df[date_col] \u001b[39m-\u001b[39m timedelta(days \u001b[39m=\u001b[39m df[date_col]\u001b[39m.\u001b[39mweekday())\n",
      "File \u001b[0;32m~/anaconda3/envs/amltesting/lib/python3.8/site-packages/pyspark/pandas/frame.py:11888\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m  11886\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc[:, key]\n\u001b[1;32m  11887\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m> 11888\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m  11889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, key)\n\u001b[1;32m  11890\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'assdf'"
     ]
    }
   ],
   "source": [
    "pre_processor_pipe(data,pre_processor_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amltesting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31fd2df77b3738767a047df5f089f6339a3269e9752fedabeb41568e1e47ecb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
